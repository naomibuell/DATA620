{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64a57085",
   "metadata": {},
   "source": [
    "# Assignment – High Frequency Words\n",
    "Authors: Naomi Buell and Richie Rivera\n",
    "\n",
    "*Please answer the following questions in an IPython Notebook, posted to GitHub.*\n",
    "1. *Choose a corpus of interest.*\n",
    "2. *How many total unique words are in the corpus? (Please feel free to define unique words in any interesting, defensible way).*\n",
    "3. *Taking the most common words, how many unique words represent half of the total words in the corpus?*\n",
    "4. *Identify the 200 highest frequency words in this corpus.*\n",
    "5. *Create a graph that shows the relative frequency of these 200 words.*\n",
    "6. *Does the observed relative frequency of these words follow Zipf’s law? Explain.*\n",
    "7. *In what ways do you think the frequency of the words in this corpus differ from “all words in all corpora.”*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d8df7f",
   "metadata": {},
   "source": [
    "## 1. Choose a corpus of interest.\n",
    "\n",
    "We chose the *One Piece Transcripts Dataset (Episodes 382–777)*, which can be found here: https://huggingface.co/datasets/mramazan/One-Piece-Transcripts-with-Character-Names-382-777. This dataset contains all dialogue lines from One Piece episodes 382 to 777.\n",
    "\n",
    "First, we import libraries and load the dataset from online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02aaeda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import string\n",
    "from collections import Counter\n",
    "# nltk.download('punkt_tab') # Download the punkt tokenizer if not already downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e4531d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"mramazan/One-Piece-Transcripts-with-Character-Names-382-777\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b4a958",
   "metadata": {},
   "source": [
    "We preview the data below. It has 5 columns: `episode`, `start`, `end`, `character`, and `text`. We will be working with the `text` column, which contains the dialogue lines spoken by characters in the episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a36e547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "episode",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "start",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "end",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "character",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "5f7dc539-94ed-4856-99bd-6707183de684",
       "rows": [
        [
         "0",
         "384",
         "0:02:31.76",
         "0:02:32.73",
         "Narrator",
         "Wealth."
        ],
        [
         "1",
         "384",
         "0:02:32.73",
         "0:02:33.99",
         "Narrator",
         "Fame."
        ],
        [
         "2",
         "384",
         "0:02:33.99",
         "0:02:34.79",
         "Narrator",
         "Power."
        ],
        [
         "3",
         "384",
         "0:02:36.01",
         "0:02:38.75",
         "Narrator",
         "The man who once owned everything,"
        ],
        [
         "4",
         "384",
         "0:02:38.75",
         "0:02:39.92",
         "Narrator",
         "The Pirate King,"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>character</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>384</td>\n",
       "      <td>0:02:31.76</td>\n",
       "      <td>0:02:32.73</td>\n",
       "      <td>Narrator</td>\n",
       "      <td>Wealth.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>384</td>\n",
       "      <td>0:02:32.73</td>\n",
       "      <td>0:02:33.99</td>\n",
       "      <td>Narrator</td>\n",
       "      <td>Fame.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>384</td>\n",
       "      <td>0:02:33.99</td>\n",
       "      <td>0:02:34.79</td>\n",
       "      <td>Narrator</td>\n",
       "      <td>Power.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>384</td>\n",
       "      <td>0:02:36.01</td>\n",
       "      <td>0:02:38.75</td>\n",
       "      <td>Narrator</td>\n",
       "      <td>The man who once owned everything,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>384</td>\n",
       "      <td>0:02:38.75</td>\n",
       "      <td>0:02:39.92</td>\n",
       "      <td>Narrator</td>\n",
       "      <td>The Pirate King,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode       start         end character  \\\n",
       "0      384  0:02:31.76  0:02:32.73  Narrator   \n",
       "1      384  0:02:32.73  0:02:33.99  Narrator   \n",
       "2      384  0:02:33.99  0:02:34.79  Narrator   \n",
       "3      384  0:02:36.01  0:02:38.75  Narrator   \n",
       "4      384  0:02:38.75  0:02:39.92  Narrator   \n",
       "\n",
       "                                 text  \n",
       "0                             Wealth.  \n",
       "1                               Fame.  \n",
       "2                              Power.  \n",
       "3  The man who once owned everything,  \n",
       "4                    The Pirate King,  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the data\n",
    "df = ds['train'].to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c09f689",
   "metadata": {},
   "source": [
    "We tokenize this `text` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "569f1194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wealth', '.', 'Fame', '.', 'Power', '.', 'The', 'man', 'who', 'once']\n"
     ]
    }
   ],
   "source": [
    "# Combine all text into a single string\n",
    "combined_text = \" \".join(df[\"text\"])\n",
    "tokens = word_tokenize(combined_text)\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b773b20",
   "metadata": {},
   "source": [
    "## 2. How many total unique words are in the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed2c34c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of first 10 unique words: ['a', 'aaaah', 'aaah', 'aah', 'aback', 'abalones', 'abandon', 'abandoned', 'abandoning', 'abandons'] ...\n",
      "Number of unique words: 12438.\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation and convert to lowercase\n",
    "unique_words = sorted(\n",
    "    set(\n",
    "        word.lower() for word in tokens\n",
    "        if word.isalpha()\n",
    "    )\n",
    ")\n",
    "print(f\"Preview of first 10 unique words: {unique_words[0:10]} ...\")\n",
    "print(f\"Number of unique words: {len(unique_words)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb53b5cb",
   "metadata": {},
   "source": [
    "There are **12,438** unique words in the corpus (this does not include punctuation, and counts unique words regardless of punctuation).\n",
    "\n",
    "## 3. Taking the most common words, how many unique words represent half of the total words in the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad4fb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total words in the corpus is: 822817, and half of the total words in the corpus is: 411408.5.\n",
      "Top 183 words appear 410551 times in the corpus.\n",
      "Top 184 words appear 411044 times in the corpus.\n",
      "Top 185 words appear 411530 times in the corpus.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"The total words in the corpus is: {len(tokens)}, and half of the total words in the corpus is: {len(tokens)/2}.\")\n",
    "\n",
    "# Count frequency of each word in tokens\n",
    "word_counts = Counter(word.lower() for word in tokens if word.isalpha())\n",
    "\n",
    "# Define thresholds (number of top words to consider)\n",
    "thresholds = [183, 184, 185]\n",
    "\n",
    "for t in thresholds:\n",
    "    most_common_words = [w for w, _ in word_counts.most_common(t)]\n",
    "    total_count = sum(word_counts[w] for w in most_common_words)\n",
    "    print(f\"Top {t} words appear {total_count} times in the corpus.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee23efc1",
   "metadata": {},
   "source": [
    "**The top 184-185 most common words** represent about half (411,044-411,530 words) of the total words in the corpus (822,817 words). \n",
    "\n",
    "## 4. Identify the 200 highest frequency words in this corpus.\n",
    "\n",
    "We print the 200 highest frequency words below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5346c4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'you', 'i', 'to', 'it', 'a', 'that', 'of', 'is', 'and', 'we', 'this', 'what', 'do', 'in', 'are', 'he', 'be', 'me', 'for', 'have', 'on', 'so', 'all', 'they', 'was', 'one', 'there', 'my', 'who', 'just', 'with', 'no', 'but', 'if', 'now', 'luffy', 'here', 'your', 'will', 'get', 'as', 'his', 'up', 'can', 'not', 'pirate', 'like', 'out', 'let', 'right', 'him', 'king', 'na', 'from', 'go', 'us', 'world', 'hey', 'pirates', 'them', 'did', 'straw', 'at', 'gon', 'about', 'how', 'has', 'time', 'man', 'too', 'hat', 'by', 'ca', 'then', 'our', 'even', 'way', 'island', 'really', 'going', 'an', 'got', 'were', 'back', 'been', 'new', 'take', 'why', 'oh', 'know', 'their', 'see', 'well', 'down', 'after', 'crew', 'sea', 'where', 'would', 'gomu', 'want', 'could', 'wo', 'come', 'only', 'yeah', 'ace', 'into', 'still', 'piece', 'please', 'guys', 'fruit', 'over', 'stop', 'wait', 'people', 'sure', 'huh', 'should', 'great', 'away', 'law', 'navy', 'make', 'think', 'must', 'when', 'alright', 'any', 'look', 'she', 'place', 'doflamingo', 'some', 'more', 'next', 'those', 'ship', 'something', 'two', 'her', 'never', 'damn', 'say', 'off', 'or', 'these', 'good', 'fight', 'again', 'die', 'before', 'already', 'roger', 'give', 'doing', 'captain', 'hurry', 'left', 'become', 'country', 'everyone', 'much', 'yes', 'had', 'fishman', 'save', 'guy', 'tell', 'kill', 'battle', 'does', 'am', 'through', 'run', 'keep', 'line', 'old', 'said', 'years', 'long', 'sorry', 'ever']\n"
     ]
    }
   ],
   "source": [
    "print(most_common_words[0:200])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
