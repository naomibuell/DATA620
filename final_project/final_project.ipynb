{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "645d8759",
   "metadata": {},
   "source": [
    "# DATA 620 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4497ea0",
   "metadata": {},
   "source": [
    "## 1. Importing Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986930f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f109a0",
   "metadata": {},
   "source": [
    "## 2. Setting up the data Pull\n",
    "\n",
    "### 2.1 Creating a process to get all Reviews\n",
    "\n",
    "We'll start this by creating functions that'll pull 100 reviews at a time and then creating a second function that'll allow us to pull these reviews in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fe1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_review_page(appid, cursor, start_date_timestamp, num_per_page, review_type, language):\n",
    "    \"\"\"\n",
    "    Helper function to fetch a single page of Steam reviews for concurrent execution.\n",
    "    Returns a tuple of (reviews, next_cursor, should_stop).\n",
    "    \"\"\"\n",
    "    url = f\"https://store.steampowered.com/appreviews/{appid}\"\n",
    "    params = {\n",
    "        'json': 1,\n",
    "        'filter': 'recent',\n",
    "        'language': language,\n",
    "        'review_type': review_type,\n",
    "        'num_per_page': num_per_page,\n",
    "        'cursor': cursor\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params, stream=True)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if data.get('success') != 1:\n",
    "            return ([], None, True) # API call failed\n",
    "\n",
    "        reviews = data.get('reviews', [])\n",
    "        next_cursor = data.get('cursor')\n",
    "        \n",
    "        # --- NEW CODE START ---\n",
    "        if reviews:\n",
    "            # The API is filtered by 'recent', so the first review is usually the youngest.\n",
    "            # We explicitly check the max just in case, but rely on the first element.\n",
    "            youngest_timestamp = reviews[0].get('timestamp_created')\n",
    "            if youngest_timestamp:\n",
    "                youngest_date = datetime.fromtimestamp(youngest_timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print(f\"Latest review date found on this page: {youngest_date}\")\n",
    "        # --- NEW CODE END ---\n",
    "        \n",
    "        filtered_reviews = []\n",
    "        should_stop = False\n",
    "\n",
    "        for review in reviews:\n",
    "            review_timestamp = review.get('timestamp_created')\n",
    "            \n",
    "            # Stop if we find a review older than the target date\n",
    "            if review_timestamp and review_timestamp < start_date_timestamp:\n",
    "                should_stop = True\n",
    "                break\n",
    "            \n",
    "            filtered_reviews.append(review)\n",
    "        \n",
    "        return (filtered_reviews, next_cursor, should_stop)\n",
    "\n",
    "    except (requests.exceptions.RequestException, json.JSONDecodeError) as e:\n",
    "        print(f\"An error occurred fetching cursor {cursor[:10]}: {e}\")\n",
    "        return ([], None, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b764f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_steam_reviews_from_2025(appid, num_per_page=100, review_type='all', language='english', max_workers=10, verbose=False):\n",
    "    \"\"\"\n",
    "    Retrieves all user reviews for a Steam App ID posted from January 1st, 2025 onward,\n",
    "    using concurrent requests.\n",
    "    \"\"\"\n",
    "    all_reviews = []\n",
    "    \n",
    "    # Define a fixed start date timestamp (January 1st, 2025)\n",
    "    start_date = datetime(2025, 1, 1)\n",
    "    start_date_timestamp = start_date.timestamp()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Starting to fetch reviews for App ID {appid}. Looking for reviews newer than {start_date.strftime('%Y-%m-%d')}.\")\n",
    "\n",
    "    # Cursors to be fetched. Start with the initial cursor '*'.\n",
    "    cursors_to_fetch = ['*']\n",
    "    \n",
    "    # Cursors that are currently being processed or scheduled.\n",
    "    scheduled_cursors = set()\n",
    "    \n",
    "    # We will use a flag to signal when we've hit the chronological limit\n",
    "    stop_fetching = False\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        \n",
    "        futures = {} # Maps future objects to the cursor they are processing\n",
    "\n",
    "        while cursors_to_fetch or futures:\n",
    "            \n",
    "            # 1. Schedule new jobs up to the max_workers limit\n",
    "            while cursors_to_fetch and len(futures) < max_workers:\n",
    "                cursor = cursors_to_fetch.pop(0)\n",
    "                if cursor not in scheduled_cursors:\n",
    "                    if verbose:\n",
    "                        print(f\"Scheduling page with cursor: {cursor[:10]}...\")\n",
    "                    future = executor.submit(\n",
    "                        fetch_review_page, \n",
    "                        appid, cursor, start_date_timestamp, num_per_page, review_type, language\n",
    "                    )\n",
    "                    futures[future] = cursor\n",
    "                    scheduled_cursors.add(cursor) # Mark as scheduled\n",
    "\n",
    "            # If no jobs are scheduled and none are running, break\n",
    "            if not futures:\n",
    "                break\n",
    "                \n",
    "            # 2. Process results as they complete\n",
    "            completed_futures = []\n",
    "            for future in futures:\n",
    "                if future.done():\n",
    "                    try:\n",
    "                        reviews, next_cursor, should_stop_page = future.result()\n",
    "                        \n",
    "                        if should_stop_page:\n",
    "                            stop_fetching = True\n",
    "                            \n",
    "                        # Only append if we haven't hit the overall stop flag\n",
    "                        if not stop_fetching:\n",
    "                            all_reviews.extend(reviews)\n",
    "                            \n",
    "                        # If a next cursor is provided, and we're not stopping, add it to the queue\n",
    "                        if next_cursor and not stop_fetching and next_cursor not in scheduled_cursors:\n",
    "                            cursors_to_fetch.append(next_cursor)\n",
    "                        \n",
    "                        if verbose:\n",
    "                            print(f\"Page processed. Total reviews: {len(all_reviews)}. Next cursor: {next_cursor[:10] if next_cursor else 'None'}. Stop flag: {should_stop_page}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Job failed for cursor {futures[future][:10]}: {e}\")\n",
    "                    \n",
    "                    completed_futures.append(future)\n",
    "            \n",
    "            # Remove completed jobs from the dictionary\n",
    "            for future in completed_futures:\n",
    "                del futures[future]\n",
    "\n",
    "            if stop_fetching:\n",
    "                print(\"Chronological limit reached by a worker. Shutting down executor.\")\n",
    "                # Cancel pending jobs and stop scheduling new ones\n",
    "                for future in futures:\n",
    "                    future.cancel()\n",
    "                break\n",
    "\n",
    "            # Add a small delay to avoid busy-waiting, but be mindful of Steam's rate limit\n",
    "            # Note: Parallel fetching already means higher rate, so the inherent delay \n",
    "            # while waiting for results helps, but we might need a more controlled throttling \n",
    "            # if this were a production system.\n",
    "            time.sleep(0.001)\n",
    "\n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b09391",
   "metadata": {},
   "source": [
    "### 2.2 Creating a process to get all Steam App IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23b193a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_steam_appids():\n",
    "    \"\"\"\n",
    "    Fetches the list of all Steam applications and their App IDs using the\n",
    "    GetAppList API endpoint.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dict has keys 'appid' and 'name'.\n",
    "        Returns an empty list if the API call fails.\n",
    "    \"\"\"\n",
    "    url = \"https://api.steampowered.com/ISteamApps/GetAppList/v2/\"\n",
    "    \n",
    "    print(\"ðŸš€ Fetching complete list of all Steam applications. This may take a moment...\")\n",
    "    \n",
    "    try:\n",
    "        # Steam's GetAppList is a public, unauthenticated endpoint\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status() # Raise an exception for bad status codes\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        # Navigate the JSON structure to the list of apps\n",
    "        apps = data.get('applist', {}).get('apps', [])\n",
    "        \n",
    "        print(f\"âœ… Successfully retrieved {len(apps):,} applications.\")\n",
    "        return apps\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"âŒ API request failed: {e}\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"âŒ Failed to decode JSON response.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f42820",
   "metadata": {},
   "source": [
    "## 3. Retrieve Steam Reviews\n",
    "### 3.1 Getting Steam App IDs\n",
    "\n",
    "In order to get our data, we'll first need to get all the Steam App IDs. We can do this by pulling from the Steam API.\n",
    "\n",
    "Once we have the App IDs, we can then use those to pull the reviews for each game.\n",
    "\n",
    "In order to make sure we don't do this more than necessary, we'll save the App IDs to a file so we can reuse them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0f82cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Fetching complete list of all Steam applications. This may take a moment...\n",
      "âœ… Successfully retrieved 276,357 applications.\n",
      "Game \"Team Fortress 2\" data already downloaded. Skipping.\n",
      "\t\t********************************************\n",
      "\t\t*  Fetching reviews for 'Counter-Strike 2  *\n",
      "\t\t********************************************\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DATA_FOLDER' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m downloading_print_line = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFetching reviews for \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msteam_app[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(downloading_print_line)+\u001b[32m6\u001b[39m)*\u001b[33m'\u001b[39m\u001b[33m*\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m*  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdownloading_print_line\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  *\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(downloading_print_line)+\u001b[32m6\u001b[39m)*\u001b[33m'\u001b[39m\u001b[33m*\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m reviews = \u001b[43mget_steam_reviews_from_2025\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mappid\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteam_app\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mappid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_per_page\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreview_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mall\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43menglish\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     32\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Save the reviews to a JSON file\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_filepath, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mget_steam_reviews_from_2025\u001b[39m\u001b[34m(appid, num_per_page, review_type, language, max_workers, verbose)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mRetrieves all user reviews for a Steam App ID posted from January 1st, 2025 onward,\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03musing ThreadPoolExecutor for concurrency with file existence check.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m \u001b[33;03m    list: A list of review dictionaries from the specified date onward.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m all_reviews = []\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m filename = os.path.join(\u001b[43mDATA_FOLDER\u001b[49m, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mappid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 1. File Existence Check\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(filename):\n",
      "\u001b[31mNameError\u001b[39m: name 'DATA_FOLDER' is not defined"
     ]
    }
   ],
   "source": [
    "# Read in the pickled list of Steam App IDs of the top 100 games\n",
    "with open('top_100_steam_app_ids.pkl', 'rb') as f:\n",
    "    top_100 = pickle.load(f)\n",
    "\n",
    "# Iterate over all Steam apps and fetch reviews for the top 100\n",
    "for steam_app in get_all_steam_appids():\n",
    "    \n",
    "    \n",
    "    # Skip apps not in the top 100\n",
    "    if steam_app['appid'] not in top_100:\n",
    "        continue\n",
    "    else:\n",
    "        # Prepare the save filepath\n",
    "        save_filename = f\"{steam_app['appid']}.json\"\n",
    "        save_filepath = f\"data/{save_filename}\"\n",
    "        \n",
    "        # Check if the file already exists\n",
    "        if os.path.exists(save_filepath):\n",
    "            print(f'Game \"{steam_app['name']}\" data already downloaded. Skipping.')\n",
    "        \n",
    "        # If not, fetch and save the reviews\n",
    "        else:\n",
    "            downloading_print_line = f\"Fetching reviews for '{steam_app['name']}\"\n",
    "            \n",
    "            print(f\"\\t\\t{(len(downloading_print_line)+6)*'*'}\\n\\t\\t*  {downloading_print_line}  *\\n\\t\\t{(len(downloading_print_line)+6)*'*'}\")\n",
    "            reviews = get_steam_reviews_from_2025(\n",
    "                appid=steam_app['appid'],\n",
    "                num_per_page=100,\n",
    "                review_type='all',\n",
    "                language='english',\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            # Save the reviews to a JSON file\n",
    "            with open(save_filepath, 'w', encoding='utf-8') as f:\n",
    "                json.dump(reviews, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89336c4a",
   "metadata": {},
   "source": [
    "### 3.2 Pulling Reviews in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf9618a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc4de625",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a78309",
   "metadata": {},
   "source": [
    "### 4.1 Data Prep & Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004426f5",
   "metadata": {},
   "source": [
    "### 4.2 Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341f2f1e",
   "metadata": {},
   "source": [
    "## 5. Network Analysis for Reviewers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33edfc2c",
   "metadata": {},
   "source": [
    "### 5.1 Network Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b81461d",
   "metadata": {},
   "source": [
    "### 5.2 Core Network Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data620env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
