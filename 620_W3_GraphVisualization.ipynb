{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "479b90d1",
   "metadata": {},
   "source": [
    "# Week Three - Assignment Graph Visualization\n",
    "\n",
    "Authors: Naomi Buell and Richie Rivera\n",
    "\n",
    "## Instructions\n",
    "\n",
    "*1. Load a graph database of your choosing from a text file or other source.  If you take a large network dataset from the web (such as from [Stanford Large Network Dataset Collection](https://snap.stanford.edu/data/)), please feel free at this point to load just a small subset of the nodes and edges.*\n",
    "\n",
    "*2. Create basic analysis on the graph, including the graphâ€™s diameter, and at least one other metric of your choosing. You may either code the functions by hand (to build your intuition and insight), or use functions in an existing package.*\n",
    "\n",
    "*3. Use a visualization tool of your choice (Neo4j, Gephi, etc.) to display information.*\n",
    "\n",
    "*4. Please record a short video (~ 5 minutes), and submit a link to the video in advance of our meet-up.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab6a2a9",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "For this assignment, we use the [wikispeedia](https://snap.stanford.edu/data/wikispeedia.html) dataset from the Stanford Large Network Dataset Collection [1, 2]. This dataset contains human navigation paths on Wikipedia, collected through the human-computation game Wikispeedia. In Wikispeedia, users are asked to navigate from a given source to a given target article, by only clicking Wikipedia links. A condensed version of Wikipedia (4,604 articles) is used. In addition to the navigation paths, we provide the full HTML package of this version of Wikipedia, as well as all articles in plaintext."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e2a3f2",
   "metadata": {},
   "source": [
    "## Step 1: Load Graph Database\n",
    "\n",
    "First, we import libraries and load the dataset into a graph object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8da86ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import networkx as nx\n",
    "import zipfile\n",
    "import os\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e7aa35d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wikipedia\\\\musae-wiki_edges.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m         zip_ref\u001b[38;5;241m.\u001b[39mextractall(extract_dir)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load the graph from the extracted edge list file\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_edgelist\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_list_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_using\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodetype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<class 'networkx.utils.decorators.argmap'> compilation 6:3\u001b[0m, in \u001b[0;36margmap_read_edgelist_1\u001b[1;34m(path, comments, delimiter, create_using, nodetype, data, edgetype, encoding, backend, **backend_kwargs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbz2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\networkx\\utils\\decorators.py:194\u001b[0m, in \u001b[0;36mopen_file.<locals>._open_file\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;66;03m# could be None, or a file handle, in which case the algorithm will deal with it\u001b[39;00m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m path, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m fobj \u001b[38;5;241m=\u001b[39m \u001b[43m_dispatch_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fobj, \u001b[38;5;28;01mlambda\u001b[39;00m: fobj\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wikipedia\\\\musae-wiki_edges.txt'"
     ]
    }
   ],
   "source": [
    "# Download and extract the dataset if not already present\n",
    "url = \"https://snap.stanford.edu/data/wikipedia.zip\"\n",
    "zip_path = \"wikipedia.zip\"\n",
    "extract_dir = \"wikipedia\"\n",
    "\n",
    "if not os.path.exists(extract_dir):\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "\n",
    "# Load the graph from the extracted edge list file\n",
    "g = nx.read_edgelist(edge_list_path, create_using=nx.Graph(), nodetype=int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a948779",
   "metadata": {},
   "source": [
    "## Step 2: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0208cb1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70f5ae15",
   "metadata": {},
   "source": [
    "## Step 3: Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd31ec8",
   "metadata": {},
   "source": [
    "## Step 4: Video\n",
    "\n",
    "Link to presentation video: [insert]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a863ca1e",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Robert West and Jure Leskovec: Human Wayfinding in Information Networks. 21st International World Wide Web Conference (WWW), 2012.\n",
    "\n",
    "[2] Robert West, Joelle Pineau, and Doina Precup: Wikispeedia: An Online Game for Inferring Semantic Distances between Concepts. 21st International Joint Conference on Artificial Intelligence (IJCAI), 2009."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
